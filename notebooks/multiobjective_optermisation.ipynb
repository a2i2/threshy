{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import autograd.numpy as anp\n",
    "\n",
    "\n",
    "from pymoo.model.evaluator import Evaluator\n",
    "from pymoo.model.problem import Problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.algorithms.nsga2 import NSGA2\n",
    "from pymoo.algorithms.rnsga3 import RNSGA3\n",
    "from pymoo.algorithms.so_gradient_descent import GradientDescent\n",
    "from pymoo.model.repair import Repair\n",
    "from pymoo.factory import *\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, brier_score_loss, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Find an example for multi-class classification (not multi-label)\n",
    "- Decide if we support REJECTS in the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/mailguard-labeled-results.csv\")\n",
    "\n",
    "inputs = {\n",
    "    \"id_column\": \"mid\",\n",
    "    \"ground_truth_column\": \"ground_truth\",\n",
    "    \"reject_label\": \"REJECT\",\n",
    "    \"min\": 0,\n",
    "    \"max\": 1,\n",
    "    \n",
    "    \"probability_column\" : \"probabilities\",  # Optional???\n",
    "    \"target_label\" : \"spam\",                 # Optional???\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/predictions-email-classifier-percept_20191014-161825.csv\")\n",
    "inputs = {\n",
    "    \"id_column\": \"email_id\",\n",
    "    \"ground_truth_column\": \"work_type\",\n",
    "    \"reject_label\": \"REJECT\",\n",
    "    \"min\": 0,\n",
    "    \"max\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_are_unique(df, inputs):\n",
    "    return len(df[inputs[\"id_column\"]].unique()) == len(df[inputs[\"id_column\"]])\n",
    "\n",
    "def retrieve_labels(df, inputs):\n",
    "    return df[inputs[\"ground_truth_column\"]].str.strip().sort_values().unique()\n",
    "\n",
    "def rejects_are_present(df, inputs):\n",
    "    return df[inputs[\"ground_truth_column\"]].str.strip().str.contains(inputs[\"reject_label\"]).any()\n",
    "\n",
    "def check_labels_have_columns(df, labels):\n",
    "    return len(set(df.columns) & set(labels)) == len(labels)\n",
    "\n",
    "def normalise_probs_in_place(df, inputs, labels):\n",
    "    if inputs[\"min\"] == 0 and inputs[\"max\"] == 100:\n",
    "        for label in labels:\n",
    "            if label == inputs[\"reject_label\"]:\n",
    "                continue\n",
    "            if (df[label] < 1).any() and (df[label] >= 0).any():\n",
    "                return \n",
    "            df[label] = df[label] / 100      \n",
    "    elif inputs[\"min\"] == 0 and inputs[\"max\"] == 1:\n",
    "        # TODO: Check that the provided constraints are not violated \n",
    "        return\n",
    "    else:\n",
    "        raise ValueError(\"Normalisation rule not specified\")\n",
    "    \n",
    "def prepare_labels(df, inputs):\n",
    "    labels = retrieve_labels(df, inputs)\n",
    "\n",
    "    if \"target_label\" in inputs:\n",
    "        labels = list(filter(lambda x: x == inputs[\"target_label\"], labels))\n",
    "    \n",
    "    if not check_labels_have_columns(df, labels) and not \"probability_column\" in inputs:\n",
    "        raise ValueError(\"Labels do not have column names for probabilities\")   \n",
    "        \n",
    "    return sorted(labels)\n",
    "\n",
    "\n",
    "def prepare_ground_truth(df, inputs, mapping):\n",
    "    ids = []\n",
    "    truth_columns = inputs[\"ground_truth_column\"]\n",
    "\n",
    "    if not \"target_label\" in inputs:\n",
    "        all_labels = []\n",
    "        for a_id in df[inputs[\"id_column\"]].unique():\n",
    "            ground_truth_labels = df[df[inputs[\"id_column\"]] == a_id][truth_columns]\n",
    "            indexes = [mapping[s.strip()] for s in list(ground_truth_labels)]     \n",
    "            ground_truth = np.zeros(len(mapping), dtype=int)\n",
    "            ground_truth[indexes] = 1\n",
    "            ids.append(a_id)\n",
    "            all_labels.append(ground_truth)\n",
    "        columns = list(mapping.keys())\n",
    "        ground_truth = pd.DataFrame(all_labels)\n",
    "        ground_truth.columns = columns\n",
    "    else:\n",
    "        ids = df[inputs[\"id_column\"]].unique()\n",
    "        ground_truth = pd.DataFrame()\n",
    "        ground_truth[inputs[\"target_label\"]] = (df[truth_columns] == inputs[\"target_label\"]) * 1\n",
    "      \n",
    "    ground_truth[\"id\"] = ids    \n",
    "    return ground_truth\n",
    "\n",
    "def derive_probabilities(df, inputs):\n",
    "    if \"probability_column\" in inputs:\n",
    "        probabilities = pd.DataFrame()\n",
    "        probabilities[inputs[\"target_label\"]] = df[inputs[\"probability_column\"]]\n",
    "        probabilities[\"id\"] = df[inputs[\"id_column\"]]\n",
    "    else:\n",
    "        labels = list(retrieve_labels(df, inputs))\n",
    "        labels.insert(0, inputs[\"id_column\"])\n",
    "        probabilities = df[labels]\n",
    "        probabilities = probabilities.rename(columns={inputs[\"id_column\"]: \"id\"})\n",
    "    return probabilities.drop_duplicates(\"id\")\n",
    "\n",
    "def thres(x, lower, upper):\n",
    "    not_match = np.less(x, lower)\n",
    "    match = np.greater_equal(x, upper)\n",
    "    rejects = ~np.logical_xor(not_match, match)\n",
    "    return np.stack([not_match, match, rejects])\n",
    "\n",
    "def matches(x):\n",
    "    return np.where(x == True)\n",
    "    \n",
    "def calculate_thresholds(labels, probabilities, thresholds):\n",
    "    predictions = pd.DataFrame()\n",
    "    for l in labels:\n",
    "        probs = probabilities[l]\n",
    "        lower = thresholds[l][\"lower\"]\n",
    "        upper = thresholds[l][\"upper\"]\n",
    "        if lower > upper:\n",
    "            raise ValueError(\"Lower %f should be less than %f\" %(lower, upper))\n",
    "        results = matches(thres(probs, lower, upper))\n",
    "        sorted_index = np.argsort(results)[1]\n",
    "        predictions[l] = results[0][sorted_index]\n",
    "    predictions[\"id\"] = df[inputs[\"id_column\"]]\n",
    "    return predictions\n",
    "\n",
    "def calculate_all_confusion_matricies(ground_truth, thresholded, labels):\n",
    "    all_matrices = []\n",
    "    for l in labels:\n",
    "        matrix = confusion_matrix(ground_truth[l], thresholded[l])\n",
    "        if np.shape(matrix) < (3,3):\n",
    "            matrix = np.c_[matrix, np.zeros(2)]\n",
    "            matrix = np.r_[matrix, [np.zeros(3)]]   \n",
    "        all_matrices.append(matrix)\n",
    "    results = np.array(all_matrices, dtype=np.int)\n",
    "    \n",
    "    # Column order: matches, not match, rejects\n",
    "    results[:,:,[0,1]] = results[:,:,[1, 0]]\n",
    "    \n",
    "    # Row order: matches, not match, rejects\n",
    "    results[:,[0,1], :] = results[:,[1, 0], :]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = prepare_labels(df, inputs)\n",
    "mapping = {label: i for i, label in enumerate(labels)}\n",
    "normalise_probs_in_place(df, inputs, labels)\n",
    "\n",
    "ground_truth = prepare_ground_truth(df, inputs, mapping)\n",
    "probabilities = derive_probabilities(df, inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5366, 5366, 5366)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = {}\n",
    "for l in labels:\n",
    "    thresholds[l] = {\"lower\": 0.60, \"upper\": 0.70}\n",
    "\n",
    "thresholded = calculate_thresholds(labels, probabilities, thresholds)\n",
    "\n",
    "len(thresholded), len(probabilities), len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[120, 109, 33],\n",
       "        [28, 5017, 59],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0.0, 3.0, 0.0],\n",
       "        [0.0, 5363.0, 0.0],\n",
       "        [0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[78, 46, 9],\n",
       "        [10, 5211, 12],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[53, 211, 45],\n",
       "        [55, 4897, 105],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[502, 239, 105],\n",
       "        [158, 4234, 128],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[230, 68, 19],\n",
       "        [21, 5003, 25],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[258, 68, 35],\n",
       "        [43, 4920, 42],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 14, 1],\n",
       "        [0, 5351, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2119, 278, 120],\n",
       "        [190, 2524, 135],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[606, 158, 75],\n",
       "        [55, 4419, 53],\n",
       "        [0, 0, 0]]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_matrices = calculate_all_confusion_matricies(ground_truth, thresholded, labels)\n",
    "all_matrices\n",
    "#thresholded[labels[0]][thresholded[labels[0]] == 0][ground_truth[labels[0]] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_bias</th>\n",
       "      <th>reject_count</th>\n",
       "      <th>ground_truth_counts</th>\n",
       "      <th>prediction_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>add_funds</th>\n",
       "      <td>0.435115</td>\n",
       "      <td>92</td>\n",
       "      <td>262</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advice</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.338346</td>\n",
       "      <td>21</td>\n",
       "      <td>133</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_dont_understand_whats_happening</th>\n",
       "      <td>0.650485</td>\n",
       "      <td>150</td>\n",
       "      <td>309</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>info</th>\n",
       "      <td>0.219858</td>\n",
       "      <td>233</td>\n",
       "      <td>846</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invest</th>\n",
       "      <td>0.208202</td>\n",
       "      <td>44</td>\n",
       "      <td>317</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setup</th>\n",
       "      <td>0.166205</td>\n",
       "      <td>77</td>\n",
       "      <td>361</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update</th>\n",
       "      <td>0.082638</td>\n",
       "      <td>255</td>\n",
       "      <td>2517</td>\n",
       "      <td>2309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>withdraw</th>\n",
       "      <td>0.212157</td>\n",
       "      <td>128</td>\n",
       "      <td>839</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   label_bias reject_count  \\\n",
       "add_funds                            0.435115           92   \n",
       "advice                               1.000000            0   \n",
       "claim                                0.338346           21   \n",
       "i_dont_understand_whats_happening    0.650485          150   \n",
       "info                                 0.219858          233   \n",
       "invest                               0.208202           44   \n",
       "setup                                0.166205           77   \n",
       "transfer                             1.000000            1   \n",
       "update                               0.082638          255   \n",
       "withdraw                             0.212157          128   \n",
       "\n",
       "                                   ground_truth_counts  prediction_counts  \n",
       "add_funds                                          262                148  \n",
       "advice                                               3                  0  \n",
       "claim                                              133                 88  \n",
       "i_dont_understand_whats_happening                  309                108  \n",
       "info                                               846                660  \n",
       "invest                                             317                251  \n",
       "setup                                              361                301  \n",
       "transfer                                            15                  0  \n",
       "update                                            2517               2309  \n",
       "withdraw                                           839                661  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_matches = thresholded[thresholded[labels] == 1][labels].count()\n",
    "ground_truth_matches = ground_truth[ground_truth[labels] == 1][labels].count()\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results[\"label_bias\"] = 1- threshold_matches / ground_truth_matches\n",
    "results[\"reject_count\"] = all_matrices[:,:,2].sum(axis=1, dtype=np.int)\n",
    "results[\"ground_truth_counts\"] = ground_truth_matches\n",
    "results[\"prediction_counts\"] = threshold_matches\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'true_matches': 3966.0,\n",
       " 'false_matches': 560.0,\n",
       " 'missed_matches': 1194.0,\n",
       " 'rejects': 1001.0,\n",
       " 'totals': 5366}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_results = {\n",
    "    \"true_matches\" : all_matrices[:,0][:,0].sum(),\n",
    "    \"false_matches\": all_matrices[:,1][:,0].sum(),\n",
    "    \"missed_matches\": all_matrices[:,0][:,1].sum(),\n",
    "    \"rejects\": all_matrices[:,:,2].sum(),\n",
    "    \"totals\": len(ground_truth)\n",
    "}\n",
    "\n",
    "summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation of the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = {\n",
    "    \"true_matches\": 1000,\n",
    "    \"false_matches\": 3000,\n",
    "    \"missed_matches\": 500,\n",
    "    \"rejects\": 500,\n",
    "    \"portion_size\" : 1000,\n",
    "    \"estimate_quantity\":10000\n",
    "}\n",
    "\n",
    "\n",
    "labels = prepare_labels(df, inputs)\n",
    "\n",
    "mapping = {label: i for i, label in enumerate(labels)}\n",
    "normalise_probs_in_place(df, inputs, labels)\n",
    "\n",
    "ground_truth = prepare_ground_truth(df, inputs, mapping)\n",
    "probabilities = derive_probabilities(df, inputs) \n",
    "\n",
    "\n",
    "ground_truth_matches = ground_truth[ground_truth[labels] == 1][labels].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matches(x, lower, upper):\n",
    "    not_match = np.less(x, lower)\n",
    "    match = np.greater_equal(x, upper)\n",
    "    rejects = ~np.logical_xor(not_match, match)\n",
    "    return np.stack([not_match, match, rejects])\n",
    "\n",
    "def apply_thresholds(probabilities, lower, upper):\n",
    "    temp = calculate_matches(probabilities, lower, upper)\n",
    "    def value(x):\n",
    "        return np.where(x == True)[0][0]\n",
    "    return np.apply_along_axis(value, 0, temp)\n",
    " \n",
    "def calculate_confusion_matrices(gt, pred):\n",
    "    result = np.zeros((gt.shape[1],3,3), dtype=np.int)\n",
    "    for i,(x,y) in enumerate(zip(gt.T,pred.T)):\n",
    "        matrix = confusion_matrix(x,y)\n",
    "        if np.shape(matrix) < (3,3):\n",
    "            matrix = np.c_[matrix, np.zeros(2)]\n",
    "            matrix = np.r_[matrix, [np.zeros(3)]]\n",
    "        elif np.shape(matrix) > (3, 3):\n",
    "            raise ValueError(\"Matrix should be 3x3, error in input labels\")\n",
    "\n",
    "        # Column order: matches, not match, rejects\n",
    "        matrix[:,[0,1]] = matrix[:,[1, 0]]\n",
    "        # Row order: matches, not match, rejects \n",
    "        matrix[[0,1], :] = matrix[[1, 0], :]\n",
    "\n",
    "        result[i] = matrix\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_objective(probabilities, ground_truth, lower_thresholds, upper_thresholds):\n",
    "    thres = apply_thresholds(probabilities, lower_thresholds, upper_thresholds)  \n",
    "    all_results = calculate_confusion_matrices(ground_truth, thres)\n",
    "    return np.array([all_results[:,0][:,0].sum(),\n",
    "            all_results[:,1][:,0].sum(),\n",
    "            all_results[:,0][:,1].sum(),\n",
    "            all_results[:,:,2].sum()])\n",
    "\n",
    "\n",
    "\n",
    "np_probs = probabilities[labels].to_numpy()    \n",
    "gt = ground_truth[labels].to_numpy()\n",
    "\n",
    "full_range = np.arange(0,1,0.01)\n",
    "threshold_values = np.full((100,10), 0.51, dtype=np.int)\n",
    "\n",
    "# TODO: Finish implementing optmisation\n",
    "#threshold_values[:,8] = full_range\n",
    "\n",
    "# r = np.zeros((100, 4), np.int)\n",
    "\n",
    "# iterations = 100\n",
    "# history = []\n",
    "# best = ()\n",
    "\n",
    "# new_threshold = anp.full(10, 0.51, dtype=np.int)\n",
    "\n",
    "# while iterations:\n",
    "#     iterations += -1\n",
    "\n",
    "#     new_score = get_objective(np_probs, gt, new_threshold, new_threshold)\n",
    "#     new_score[:,1:] = new_score[:,1:] * -1\n",
    "#     score = np.sum(r,1)\n",
    "#     if best[0] < score:\n",
    "#         best = (score, new_threshold)\n",
    "            \n",
    "#     history.append((score, new_threshold))\n",
    "#https://gpflowopt.readthedocs.io/en/latest/notebooks/multiobjective.html#\n",
    "\n",
    "# get_objective(np_probs, gt, threshold_values[i], threshold_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14373d810>"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(r).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pymoo optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyRepair(Repair):\n",
    "\n",
    "    def _do(self, problem, pop, **kwargs):\n",
    "        for i in range(len(pop)):\n",
    "            x = pop[i].X\n",
    "            if len(x) % 2 == 0:\n",
    "                for j in range(0, len(x), 2):                    \n",
    "                    if x[j] > x[j + 1]:\n",
    "                        x[j], x[j + 1] = x[j + 1], x[j]\n",
    "        return pop\n",
    "    \n",
    "    \n",
    "class FindThresholds(Problem):\n",
    "    def __init__(self, number_of_thresholds, labels):\n",
    "        if not len(labels) == number_of_thresholds and not len(labels) * 2 == number_of_thresholds:\n",
    "            raise ValueError(\"Number of thresholds must be same size as labels or twice the size\")\n",
    "        \n",
    "        self.labels = labels    \n",
    "        self.number_of_thresholds = number_of_thresholds            \n",
    "        super().__init__(n_var=number_of_thresholds, \n",
    "                         n_constr=0, \n",
    "                         n_obj=4, \n",
    "                         xl=anp.zeros((number_of_thresholds,), dtype=anp.double), \n",
    "                         xu=anp.ones((number_of_thresholds,), dtype=anp.double),  \n",
    "                         type_var=anp.double, \n",
    "                         elementwise_evaluation=True)\n",
    "    \n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        f0 = []\n",
    "        f1 = []\n",
    "        f2 = []\n",
    "        f3 = []\n",
    "        \n",
    "        if self.number_of_thresholds == 1:        \n",
    "            thresholds = {l: {\"lower\": X[0], \"upper\":X[0]} for l in self.labels} \n",
    "        elif self.number_of_thresholds == len(self.labels) ** 2:\n",
    "            thresholds = {}\n",
    "            for label_index, i in enumerate(range(0, self.number_of_thresholds, 2)):\n",
    "                thresholds[self.labels[label_index]] = {\"lower\": X[i], \"upper\":X[i+1]}\n",
    "        else:\n",
    "            thresholds = {}\n",
    "            for i in range(0, self.number_of_thresholds):\n",
    "                thresholds[self.labels[i]] = {\"lower\": X[i], \"upper\":X[i]} \n",
    "\n",
    "        thresholded = calculate_thresholds(self.labels, probabilities, thresholds)\n",
    "        all_matrices = calculate_all_confusion_matricies(ground_truth, thresholded, self.labels)\n",
    "\n",
    "        # true matches\n",
    "        f0.append(all_matrices[:,0][:,0].sum() * -1)   # Maximise the # of true matches\n",
    "\n",
    "        # false matches\n",
    "        f1.append(all_matrices[:,1][:,0].sum())\n",
    "\n",
    "        # missed matches\n",
    "        f2.append(all_matrices[:,0][:,1].sum())\n",
    "\n",
    "        # rejects\n",
    "        f3.append(all_matrices[:,:,2].sum())\n",
    "            \n",
    "        out[\"F\"] = anp.column_stack([f0, f1, f2, f3]).astype(anp.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiled modules for significant speedup can not be used!\n",
      "https://pymoo.org/installation.html#installation\n",
      "\n",
      "To disable this warning:\n",
      "from pymoo.configuration import Configuration\n",
      "Configuration.show_compile_hint = False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pymoo/algorithms/rnsga3.py:142: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  unit_ref_points = (self.ref_points - self.ideal_point) / (self.nadir_point - self.ideal_point)\n",
      "/opt/conda/lib/python3.7/site-packages/pymoo/algorithms/rnsga3.py:239: RuntimeWarning: invalid value encountered in multiply\n",
      "  l = l * d\n",
      "/opt/conda/lib/python3.7/site-packages/pymoo/algorithms/rnsga3.py:204: RuntimeWarning: invalid value encountered in greater\n",
      "  if not (ref_dir_for_aspiration_point > 0).min():\n",
      "/opt/conda/lib/python3.7/site-packages/pymoo/algorithms/rnsga3.py:205: RuntimeWarning: invalid value encountered in less\n",
      "  ref_dir_for_aspiration_point[ref_dir_for_aspiration_point < 0] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "n_gen | n_eval \n",
      "======================================================================\n",
      "1     | 60     \n",
      "2     | 120    \n",
      "3     | 180    \n",
      "4     | 240    \n",
      "5     | 300    \n",
      "6     | 360    \n",
      "7     | 420    \n",
      "8     | 480    \n",
      "9     | 540    \n",
      "10    | 600    \n",
      "11    | 660    \n",
      "12    | 720    \n",
      "13    | 780    \n",
      "14    | 840    \n",
      "15    | 900    \n",
      "16    | 960    \n",
      "17    | 1020   \n",
      "18    | 1080   \n",
      "19    | 1140   \n",
      "20    | 1200   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "algorithm = NSGA2(\n",
    "    pop_size=40,\n",
    "    n_offsprings=10,\n",
    "    repair=MyRepair(),\n",
    "    sampling=get_sampling(\"real_random\"),\n",
    "    crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "    mutation=get_mutation(\"real_pm\", eta=20),\n",
    "    eliminate_duplicates=True\n",
    ")\n",
    "\n",
    "ref_points = np.array([[0.5, 0.5, 0.5, 0.5]])\n",
    "algorithm = RNSGA3(\n",
    "    ref_points=ref_points,\n",
    "    pop_per_ref_point=56,\n",
    "    mu=0.1)\n",
    "\n",
    "\n",
    "problem = FindThresholds(10, labels)\n",
    "\n",
    "res = minimize(problem,\n",
    "               algorithm,\n",
    "               (\"n_gen\", 20),\n",
    "               seed=1,\n",
    "               save_history=True,\n",
    "               verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regarding decomposition: Point 0 - [-5459. 27106.   143.     0.], [0.00097333 0.84058348 0.68224328 0.28781845 0.00299394 0.6507204\n",
      " 0.15734461 0.77966049 0.02463698 0.15525791]\n"
     ]
    }
   ],
   "source": [
    "F = res.F\n",
    "\n",
    "weights = np.array([0.7,0.1,0.1,0.1])\n",
    "I = get_decomposition(\"weighted-sum\").do(F, weights).argmin()\n",
    "print(\"Best regarding decomposition: Point %s - %s, %s\" % (I, F[I], res.X[I]))\n",
    "\n",
    "# Best regarding decomposition: Point 0 - [-32.   1.  19.   0.], [0.67100345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "n_gen | n_eval  | favg         | fopt        \n",
      "======================================================================\n",
      "1     | 1       | -35.0000000000 | -35.0000000000\n",
      "2     | 2       | -35.0000000000 | -35.0000000000\n",
      "3     | 3       | -35.0000000000 | -35.0000000000\n",
      "4     | 4       | -35.0000000000 | -35.0000000000\n",
      "5     | 5       | -35.0000000000 | -35.0000000000\n",
      "6     | 6       | -35.0000000000 | -35.0000000000\n",
      "7     | 7       | -35.0000000000 | -35.0000000000\n",
      "8     | 8       | -35.0000000000 | -35.0000000000\n",
      "9     | 9       | -35.0000000000 | -35.0000000000\n",
      "10    | 10      | -35.0000000000 | -35.0000000000\n",
      "11    | 11      | -35.0000000000 | -35.0000000000\n",
      "12    | 12      | -35.0000000000 | -35.0000000000\n",
      "13    | 13      | -35.0000000000 | -35.0000000000\n",
      "14    | 14      | -35.0000000000 | -35.0000000000\n",
      "15    | 15      | -35.0000000000 | -35.0000000000\n",
      "16    | 16      | -35.0000000000 | -35.0000000000\n",
      "17    | 17      | -35.0000000000 | -35.0000000000\n",
      "18    | 18      | -35.0000000000 | -35.0000000000\n",
      "19    | 19      | -35.0000000000 | -35.0000000000\n",
      "20    | 20      | -35.0000000000 | -35.0000000000\n",
      "21    | 21      | -35.0000000000 | -35.0000000000\n",
      "22    | 22      | -35.0000000000 | -35.0000000000\n",
      "23    | 23      | -35.0000000000 | -35.0000000000\n",
      "24    | 24      | -35.0000000000 | -35.0000000000\n",
      "25    | 25      | -35.0000000000 | -35.0000000000\n",
      "26    | 26      | -35.0000000000 | -35.0000000000\n",
      "27    | 27      | -35.0000000000 | -35.0000000000\n",
      "28    | 28      | -35.0000000000 | -35.0000000000\n",
      "29    | 29      | -35.0000000000 | -35.0000000000\n",
      "30    | 30      | -35.0000000000 | -35.0000000000\n",
      "31    | 31      | -35.0000000000 | -35.0000000000\n",
      "32    | 32      | -35.0000000000 | -35.0000000000\n",
      "33    | 33      | -35.0000000000 | -35.0000000000\n",
      "34    | 34      | -35.0000000000 | -35.0000000000\n",
      "35    | 35      | -35.0000000000 | -35.0000000000\n",
      "36    | 36      | -35.0000000000 | -35.0000000000\n",
      "37    | 37      | -35.0000000000 | -35.0000000000\n",
      "38    | 38      | -35.0000000000 | -35.0000000000\n",
      "39    | 39      | -35.0000000000 | -35.0000000000\n",
      "40    | 40      | -35.0000000000 | -35.0000000000\n",
      "41    | 41      | -35.0000000000 | -35.0000000000\n",
      "42    | 42      | -35.0000000000 | -35.0000000000\n",
      "43    | 43      | -35.0000000000 | -35.0000000000\n",
      "44    | 44      | -35.0000000000 | -35.0000000000\n",
      "45    | 45      | -35.0000000000 | -35.0000000000\n",
      "46    | 46      | -35.0000000000 | -35.0000000000\n",
      "47    | 47      | -35.0000000000 | -35.0000000000\n",
      "48    | 48      | -35.0000000000 | -35.0000000000\n",
      "49    | 49      | -35.0000000000 | -35.0000000000\n",
      "50    | 50      | -35.0000000000 | -35.0000000000\n",
      "51    | 51      | -35.0000000000 | -35.0000000000\n",
      "52    | 52      | -35.0000000000 | -35.0000000000\n",
      "53    | 53      | -35.0000000000 | -35.0000000000\n",
      "54    | 54      | -35.0000000000 | -35.0000000000\n",
      "55    | 55      | -35.0000000000 | -35.0000000000\n",
      "56    | 56      | -35.0000000000 | -35.0000000000\n",
      "57    | 57      | -35.0000000000 | -35.0000000000\n",
      "58    | 58      | -35.0000000000 | -35.0000000000\n",
      "59    | 59      | -35.0000000000 | -35.0000000000\n",
      "60    | 60      | -35.0000000000 | -35.0000000000\n",
      "61    | 61      | -35.0000000000 | -35.0000000000\n",
      "62    | 62      | -35.0000000000 | -35.0000000000\n",
      "63    | 63      | -35.0000000000 | -35.0000000000\n",
      "64    | 64      | -35.0000000000 | -35.0000000000\n",
      "65    | 65      | -35.0000000000 | -35.0000000000\n",
      "66    | 66      | -35.0000000000 | -35.0000000000\n",
      "67    | 67      | -35.0000000000 | -35.0000000000\n",
      "68    | 68      | -35.0000000000 | -35.0000000000\n",
      "69    | 69      | -35.0000000000 | -35.0000000000\n",
      "70    | 70      | -35.0000000000 | -35.0000000000\n",
      "71    | 71      | -35.0000000000 | -35.0000000000\n",
      "72    | 72      | -35.0000000000 | -35.0000000000\n",
      "73    | 73      | -35.0000000000 | -35.0000000000\n",
      "74    | 74      | -35.0000000000 | -35.0000000000\n",
      "75    | 75      | -35.0000000000 | -35.0000000000\n",
      "76    | 76      | -35.0000000000 | -35.0000000000\n",
      "77    | 77      | -35.0000000000 | -35.0000000000\n",
      "78    | 78      | -35.0000000000 | -35.0000000000\n",
      "79    | 79      | -35.0000000000 | -35.0000000000\n",
      "80    | 80      | -35.0000000000 | -35.0000000000\n",
      "81    | 81      | -35.0000000000 | -35.0000000000\n",
      "82    | 82      | -35.0000000000 | -35.0000000000\n",
      "83    | 83      | -35.0000000000 | -35.0000000000\n",
      "84    | 84      | -35.0000000000 | -35.0000000000\n",
      "85    | 85      | -35.0000000000 | -35.0000000000\n",
      "86    | 86      | -35.0000000000 | -35.0000000000\n",
      "87    | 87      | -35.0000000000 | -35.0000000000\n",
      "88    | 88      | -35.0000000000 | -35.0000000000\n",
      "89    | 89      | -35.0000000000 | -35.0000000000\n",
      "90    | 90      | -35.0000000000 | -35.0000000000\n",
      "91    | 91      | -35.0000000000 | -35.0000000000\n",
      "92    | 92      | -35.0000000000 | -35.0000000000\n",
      "93    | 93      | -35.0000000000 | -35.0000000000\n",
      "94    | 94      | -35.0000000000 | -35.0000000000\n",
      "95    | 95      | -35.0000000000 | -35.0000000000\n",
      "96    | 96      | -35.0000000000 | -35.0000000000\n",
      "97    | 97      | -35.0000000000 | -35.0000000000\n",
      "98    | 98      | -35.0000000000 | -35.0000000000\n",
      "99    | 99      | -35.0000000000 | -35.0000000000\n",
      "100   | 100     | -35.0000000000 | -35.0000000000\n",
      "101   | 101     | -35.0000000000 | -35.0000000000\n",
      "102   | 102     | -35.0000000000 | -35.0000000000\n",
      "103   | 103     | -35.0000000000 | -35.0000000000\n",
      "104   | 104     | -35.0000000000 | -35.0000000000\n",
      "105   | 105     | -35.0000000000 | -35.0000000000\n",
      "106   | 106     | -35.0000000000 | -35.0000000000\n",
      "107   | 107     | -35.0000000000 | -35.0000000000\n",
      "108   | 108     | -35.0000000000 | -35.0000000000\n",
      "109   | 109     | -35.0000000000 | -35.0000000000\n",
      "110   | 110     | -35.0000000000 | -35.0000000000\n",
      "111   | 111     | -35.0000000000 | -35.0000000000\n",
      "112   | 112     | -35.0000000000 | -35.0000000000\n",
      "113   | 113     | -35.0000000000 | -35.0000000000\n",
      "114   | 114     | -35.0000000000 | -35.0000000000\n",
      "115   | 115     | -35.0000000000 | -35.0000000000\n",
      "116   | 116     | -35.0000000000 | -35.0000000000\n",
      "117   | 117     | -35.0000000000 | -35.0000000000\n",
      "118   | 118     | -35.0000000000 | -35.0000000000\n",
      "119   | 119     | -35.0000000000 | -35.0000000000\n",
      "120   | 120     | -35.0000000000 | -35.0000000000\n",
      "121   | 121     | -35.0000000000 | -35.0000000000\n",
      "122   | 122     | -35.0000000000 | -35.0000000000\n",
      "123   | 123     | -35.0000000000 | -35.0000000000\n",
      "124   | 124     | -35.0000000000 | -35.0000000000\n",
      "125   | 125     | -35.0000000000 | -35.0000000000\n",
      "126   | 126     | -35.0000000000 | -35.0000000000\n",
      "127   | 127     | -35.0000000000 | -35.0000000000\n",
      "128   | 128     | -35.0000000000 | -35.0000000000\n",
      "129   | 129     | -35.0000000000 | -35.0000000000\n",
      "130   | 130     | -35.0000000000 | -35.0000000000\n",
      "131   | 131     | -35.0000000000 | -35.0000000000\n",
      "132   | 132     | -35.0000000000 | -35.0000000000\n",
      "133   | 133     | -35.0000000000 | -35.0000000000\n",
      "134   | 134     | -35.0000000000 | -35.0000000000\n",
      "135   | 135     | -35.0000000000 | -35.0000000000\n",
      "136   | 136     | -35.0000000000 | -35.0000000000\n",
      "137   | 137     | -35.0000000000 | -35.0000000000\n",
      "138   | 138     | -35.0000000000 | -35.0000000000\n",
      "139   | 139     | -35.0000000000 | -35.0000000000\n",
      "140   | 140     | -35.0000000000 | -35.0000000000\n",
      "141   | 141     | -35.0000000000 | -35.0000000000\n",
      "142   | 142     | -35.0000000000 | -35.0000000000\n",
      "143   | 143     | -35.0000000000 | -35.0000000000\n",
      "144   | 144     | -35.0000000000 | -35.0000000000\n",
      "145   | 145     | -35.0000000000 | -35.0000000000\n",
      "146   | 146     | -35.0000000000 | -35.0000000000\n",
      "147   | 147     | -35.0000000000 | -35.0000000000\n",
      "148   | 148     | -35.0000000000 | -35.0000000000\n",
      "149   | 149     | -35.0000000000 | -35.0000000000\n",
      "150   | 150     | -35.0000000000 | -35.0000000000\n",
      "151   | 151     | -35.0000000000 | -35.0000000000\n",
      "152   | 152     | -35.0000000000 | -35.0000000000\n",
      "153   | 153     | -35.0000000000 | -35.0000000000\n",
      "154   | 154     | -35.0000000000 | -35.0000000000\n",
      "155   | 155     | -35.0000000000 | -35.0000000000\n",
      "156   | 156     | -35.0000000000 | -35.0000000000\n",
      "157   | 157     | -35.0000000000 | -35.0000000000\n",
      "158   | 158     | -35.0000000000 | -35.0000000000\n",
      "159   | 159     | -35.0000000000 | -35.0000000000\n",
      "160   | 160     | -35.0000000000 | -35.0000000000\n",
      "161   | 161     | -35.0000000000 | -35.0000000000\n",
      "162   | 162     | -35.0000000000 | -35.0000000000\n",
      "163   | 163     | -35.0000000000 | -35.0000000000\n",
      "164   | 164     | -35.0000000000 | -35.0000000000\n",
      "165   | 165     | -35.0000000000 | -35.0000000000\n",
      "166   | 166     | -35.0000000000 | -35.0000000000\n",
      "167   | 167     | -35.0000000000 | -35.0000000000\n",
      "168   | 168     | -35.0000000000 | -35.0000000000\n",
      "169   | 169     | -35.0000000000 | -35.0000000000\n",
      "170   | 170     | -35.0000000000 | -35.0000000000\n",
      "171   | 171     | -35.0000000000 | -35.0000000000\n",
      "172   | 172     | -35.0000000000 | -35.0000000000\n",
      "173   | 173     | -35.0000000000 | -35.0000000000\n",
      "174   | 174     | -35.0000000000 | -35.0000000000\n",
      "175   | 175     | -35.0000000000 | -35.0000000000\n",
      "176   | 176     | -35.0000000000 | -35.0000000000\n",
      "177   | 177     | -35.0000000000 | -35.0000000000\n",
      "178   | 178     | -35.0000000000 | -35.0000000000\n",
      "179   | 179     | -35.0000000000 | -35.0000000000\n",
      "180   | 180     | -35.0000000000 | -35.0000000000\n",
      "181   | 181     | -35.0000000000 | -35.0000000000\n",
      "182   | 182     | -35.0000000000 | -35.0000000000\n",
      "183   | 183     | -35.0000000000 | -35.0000000000\n",
      "184   | 184     | -35.0000000000 | -35.0000000000\n",
      "185   | 185     | -35.0000000000 | -35.0000000000\n",
      "186   | 186     | -35.0000000000 | -35.0000000000\n",
      "187   | 187     | -35.0000000000 | -35.0000000000\n",
      "188   | 188     | -35.0000000000 | -35.0000000000\n",
      "189   | 189     | -35.0000000000 | -35.0000000000\n",
      "190   | 190     | -35.0000000000 | -35.0000000000\n",
      "191   | 191     | -35.0000000000 | -35.0000000000\n",
      "192   | 192     | -35.0000000000 | -35.0000000000\n",
      "193   | 193     | -35.0000000000 | -35.0000000000\n",
      "194   | 194     | -35.0000000000 | -35.0000000000\n",
      "195   | 195     | -35.0000000000 | -35.0000000000\n",
      "196   | 196     | -35.0000000000 | -35.0000000000\n",
      "197   | 197     | -35.0000000000 | -35.0000000000\n",
      "198   | 198     | -35.0000000000 | -35.0000000000\n",
      "199   | 199     | -35.0000000000 | -35.0000000000\n",
      "200   | 200     | -35.0000000000 | -35.0000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = anp.array(anp.matrix(anp.arange(0,1,0.01)).T)\n",
    "\n",
    "gd = GradientDescent(anp.array([0.3]), termination=get_termination(\"n_eval\", 200))\n",
    "gd.evaluator = Evaluator()\n",
    "gd.problem = FindThresholds(1, labels)\n",
    "\n",
    "#gd.solve()\n",
    "res = minimize(FindThresholds(1, labels),\n",
    "               gd,\n",
    "               (\"n_gen\", 300),\n",
    "               seed=1,\n",
    "               save_history=True,\n",
    "               verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5602.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradientDescent2(GradientBasedAlgorithm):\n",
    "\n",
    "    def __init__(self, X, learning_rate=0.005, **kwargs) -> None:\n",
    "        super().__init__(X, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def restart(self):\n",
    "        self.learning_rate /= 2\n",
    "\n",
    "    def apply(self):\n",
    "        self.X = self.X - self.learning_rate * self.dX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
